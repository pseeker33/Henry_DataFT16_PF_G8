{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se desarrolla el ETL de los datasets de reviews de Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importacion de Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de los Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargará un json para ver su formato y realizar un primer análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos=['./Maps/Estados/review-Arizona/1.json']\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "                data.append(json.loads(line))\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna \"pics\" y estandarizamos la columna \"name\"\n",
    "df['name'] = df['name'].str.title()\n",
    "df=df.drop(['pics'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos más bibliotecas\n",
    "from datetime import datetime\n",
    "# Se crea la función para convertir marca de tiempo a fecha\n",
    "def convertir_a_fecha(timestamp_ms):\n",
    "    timestamp_seconds = timestamp_ms / 1000\n",
    "    fecha = datetime.utcfromtimestamp(timestamp_seconds).date()\n",
    "    return fecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargaremos todos los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos = ['./Maps/Estados/review-Arizona/{}.json'.format(i) for i in range(1, 15)]\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "df_arizona = pd.DataFrame(data)\n",
    "# Modificaciones adicionales\n",
    "df_arizona['name'] = df_arizona['name'].str.title()\n",
    "df_arizona = df_arizona.drop(['pics'], axis=1)\n",
    "df_arizona['time'] = df_arizona['time'].apply(convertir_a_fecha)\n",
    "# Guardar en formato Parquet\n",
    "df_arizona.to_parquet('reviews_arizona.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos = ['./Maps/Estados/review-California/{}.json'.format(i) for i in range(1, 19)]\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "df_california = pd.DataFrame(data)\n",
    "df_california['name'] = df_california['name'].str.title()\n",
    "df_california=df_california.drop(['pics'],axis=1)\n",
    "df_california['time'] = df_california['time'].apply(convertir_a_fecha)\n",
    "df_california.to_parquet('reviews_california.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos = ['./Maps/Estados/review-Florida/{}.json'.format(i) for i in range(1, 20)]\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "df_florida = pd.DataFrame(data)\n",
    "df_florida['name'] = df_florida['name'].str.title()\n",
    "df_florida=df_florida.drop(['pics'],axis=1)\n",
    "df_florida['time'] = df_florida['time'].apply(convertir_a_fecha)\n",
    "df_florida.to_parquet('reviews_florida.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos = ['./Maps/Estados/review-Illinois/{}.json'.format(i) for i in range(1, 15)]\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "df_illinois = pd.DataFrame(data)\n",
    "df_illinois['name'] = df_illinois['name'].str.title()\n",
    "df_illinois=df_illinois.drop(['pics'],axis=1)\n",
    "df_illinois['time'] = df_illinois['time'].apply(convertir_a_fecha)\n",
    "df_illinois.to_parquet('reviews_illinois.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos = ['./Maps/Estados/review-Indiana/{}.json'.format(i) for i in range(1, 16)]\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "df_indiana = pd.DataFrame(data)\n",
    "df_indiana['name'] = df_indiana['name'].str.title()\n",
    "df_indiana=df_indiana.drop(['pics'],axis=1)\n",
    "df_indiana['time'] = df_indiana['time'].apply(convertir_a_fecha)\n",
    "df_indiana.to_parquet('reviews_indiana.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos = ['./Maps/Estados/review-New_Jersey/{}.json'.format(i) for i in range(1, 14)]\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "df_new_jersey = pd.DataFrame(data)\n",
    "df_new_jersey['name'] = df_new_jersey['name'].str.title()\n",
    "df_new_jersey=df_new_jersey.drop(['pics'],axis=1)\n",
    "df_new_jersey['time'] = df_new_jersey['time'].apply(convertir_a_fecha)\n",
    "df_new_jersey.to_parquet('reviews_new_jersey.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos = ['./Maps/Estados/review-Pennsylvania/{}.json'.format(i) for i in range(1, 17)]\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "df_pennsylvania = pd.DataFrame(data)\n",
    "df_pennsylvania['name'] = df_pennsylvania['name'].str.title()\n",
    "df_pennsylvania=df_pennsylvania.drop(['pics'],axis=1)\n",
    "df_pennsylvania['time'] = df_pennsylvania['time'].apply(convertir_a_fecha)\n",
    "df_pennsylvania.to_parquet('reviews_pennsylvania.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "archivos = ['./Maps/Estados/review-Tennessee/{}.json'.format(i) for i in range(1, 13)]\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "df_tennessee = pd.DataFrame(data)\n",
    "df_tennessee['name'] = df_tennessee['name'].str.title()\n",
    "df_tennessee=df_tennessee.drop(['pics'],axis=1)\n",
    "df_tennessee['time'] = df_tennessee['time'].apply(convertir_a_fecha)\n",
    "df_tennessee.to_parquet('reviews_tennessee.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging de los Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un dataframe final uniendo todos los dataframes anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.concat([df_arizona,df_california])\n",
    "df_reviews = pd.concat([df_reviews,df_florida])\n",
    "df_reviews = pd.concat([df_reviews,df_illinois])\n",
    "df_reviews = pd.concat([df_reviews,df_indiana])\n",
    "df_reviews = pd.concat([df_reviews,df_new_jersey])\n",
    "df_reviews = pd.concat([df_reviews,df_pennsylvania])\n",
    "df_reviews = pd.concat([df_reviews,df_tennessee])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos para que todos los reviews del dataframe df_reviews estén referenciados a los restaurantes del df_restaurent. Esto lo hacemos mediante el gmap_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista con todos los valores únicos de la columna \"gmap_id\" en df_restaurant\n",
    "gmap_ids_restaurant = df_restaurant['gmap_id'].unique().tolist()\n",
    "\n",
    "# Verificar si cada gmap_id en la lista está presente en la columna \"gmap_id\" de df_reviews\n",
    "present_in_reviews = df_reviews['gmap_id'].isin(gmap_ids_restaurant)\n",
    "\n",
    "# Crear un nuevo DataFrame con las filas de df_reviews que tienen gmap_id presente en df_restaurant\n",
    "df_reviews_filtered = df_reviews[present_in_reviews]\n",
    "\n",
    "# Si deseas una lista de los gmap_id que están presentes en ambos DataFrames, puedes hacer lo siguiente\n",
    "gmap_ids_present = df_reviews_filtered['gmap_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean los dataframes finales con todos los filtros necesarios:\n",
    "* todos los negocios seleccionados son del género ocio, y para ser más precisos son de la categoría \"Restaurant\".\n",
    "* todos pertenecen a los estados de: Arizona, California, Florida, Illinois, Indiana, New Jersey, Pennsylvania y Tennessee.\n",
    "* todas las reviews perteneces a los restaurantes de dichos estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes finales\n",
    "df_reviews=df_reviews[df_reviews['gmap_id'].isin(gmap_ids_present)].reset_index(drop=True)\n",
    "df_reviews.to_parquet('reviews.parquet', index=False, engine='pyarrow')\n",
    "df_restaurant=df_restaurant[df_restaurant['gmap_id'].isin(gmap_ids_present)].reset_index(drop=True)\n",
    "df_restaurant.to_parquet('restaurants.parquet', index=False, engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
